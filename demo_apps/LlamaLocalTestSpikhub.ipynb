{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922732e-29e8-4ea7-8828-53364f5bf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all the required packages for the demo\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "# !pip install pypdf sentence-transformers chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df46d9",
   "metadata": {},
   "source": [
    "Next, initialize the langchain `CallBackManager`. This handles callbacks from Langchain and for this example we will use token-wise streaming so the answer gets generated token by token when Llama is answering your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token-wise streaming so you'll see the answer gets generated token by token when Llama is answering your question\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536c352",
   "metadata": {},
   "source": [
    "\n",
    "Set up the Llama 2 model. \n",
    "\n",
    "Replace `<path-to-llama-gguf-file>` with the path either to your downloaded quantized model file [here](https://drive.google.com/file/d/1afPv3HOy73BE2MoYCgYJvBDeQNa9rZbj/view?usp=sharing), \n",
    "\n",
    "or to the `ggml-model-q4_0.gguf` file built with the following commands:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "python3 -m pip install -r requirements.txt\n",
    "python convert.py <path_to_your_downloaded_llama-2-13b_model>\n",
    "./quantize <path_to_your_downloaded_llama-2-13b_model>/ggml-model-f16.gguf <path_to_your_downloaded_llama-2-13b_model>/ggml-model-q4_0.gguf q4_0\n",
    "\n",
    "```\n",
    "For more info see https://python.langchain.com/docs/integrations/llms/llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/mnt/DATASSD2/LargeModels/LLaMa-2/llama-2-13b-gguf/ggml-model-q4_0.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    temperature=1,\n",
    "    top_p=0.9,\n",
    "    n_ctx=10000,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# embeddings are numerical representations of the question and answer text\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# use a common text splitter to split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJEMPLO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '''\n",
    "responsabilidad social corporativa de inditex\n",
    "Colaboramos para generar un impacto positivo\n",
    "Nos encontramos ante un cambio de paradigma en el que la colaboración de diversos actores puede lograr una transformación sostenible de nuestra industria. Por ello, todos los que formamos Inditex colaboramos con universidades, organizaciones sociales, sindicatos y otras empresas para avanzar hacia un modelo circular, en el que nuestros proveedores crezcan y podamos seguir generando valor en las comunidades en las que desarrollamos nuestra actividad, con el objetivo de lograr un impacto positivo para las personas y el planeta.\n",
    "Colaboramos con nuestras personas, motor de esa transformación\n",
    "Nuestros equipos son el verdadero motor de nuestra transformación. Por eso, impulsamos oportunidades de desarrollo y formación que contribuyan a su desarrollo personal y profesional.\n",
    "Colaboramos con nuestros clientes, los grandes impulsores\n",
    "La relación con nuestros clientes es la base de nuestro modelo de negocio. Sus percepciones y comentarios son claves a la hora de impulsarnos para conseguir una industria más sostenible.\n",
    "Colaboramos con nuestros accionistas, aliados para transformar\n",
    "Potenciamos una comunicación transparente y abierta con nuestros accionistas, favoreciendo su participación en la toma de decisiones, clave para seguir avanzando en la transformación del sector.\n",
    "Colaboramos para transformar a través de una gestión sostenible de nuestros productos\n",
    "Ofrecer a nuestros clientes productos sostenibles y avanzar hacia un modelo circular es vital para conseguir una transformación de nuestra industria.\n",
    "Colaboramos para que nuestros proveedores crezcan\n",
    "Una relación fluida con nuestros proveedores es clave para que avancen con nosotros hacia una cadena de suministro más sostenible, dando respuestas a los retos y desafíos de los diferentes mercados en los que operamos.\n",
    "Colaboramos para preservar el planeta\n",
    "Nuestro compromiso con la sostenibilidad impregna todas las fases de la cadena de valor. Reducir el impacto ambiental y contribuir a una industria más sostenible son nuestros principales objetivos.\n",
    "Colaboramos para generar valor en las comunidades\n",
    "Nuestra misión trasciende al plano económico y pasa por aportar valor y contribuir al desarrollo social. Por ello, en 2021 colaboramos con 725 iniciativas sociales que beneficiaron a 2,2 millones de personas.\n",
    "Responsabilidad y transparencia fiscal\n",
    "El estricto cumplimiento de nuestras obligaciones fiscales responde también a nuestro compromiso con la creación de valor y al desarrollo de un impacto social positivo allí donde estamos presentes.\n",
    "Buen gobierno, cultura ética corporativa y sólida arquitectura de Compliance\n",
    "Transmitir la cultura ética corporativa a todos los grupos de interés resulta esencial para Inditex, que cuenta con un sistema normativo interno que favorece el desarrollo de un modelo ético, eficiente y competitivo.\n",
    "Gestión responsable del riesgo\n",
    "Una gestión eficaz del riesgo en todos los niveles del Grupo nos permite mantener un buen desempeño y genera un rendimiento empresarial sostenible a lo largo del tiempo, necesario para transformar.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"inditex.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the loaded documents into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the vector db to store all the split chunks as embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use another LangChain's chain, RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"diseña unas acciones de responsabilidad social corporativa para inditex\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJEMPLO 2\n",
    "- extrater texto de un html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://static.inditex.com/annual_report_2021/es/impacto-positivo.html\"\n",
    "url = \"https://www.linkedin.com/company/inditex/?originalSubdomain=es\"\n",
    "url = \"https://www.repsol.com/es/sostenibilidad/index.cshtml\"\n",
    "html = urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.text\n",
    "text_2 = text.strip()\n",
    "text_2 = text_2.replace(\"\\n\", \" \")\n",
    "text_2 = text_2.replace(\"\\t\", \" \")\n",
    "text_2 = text_2.replace(\"\\r\", \" \")\n",
    "text_2 = text_2.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"translate to english \" + text_2\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
